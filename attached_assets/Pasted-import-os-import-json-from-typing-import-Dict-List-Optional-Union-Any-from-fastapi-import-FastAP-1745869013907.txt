import os
import json
from typing import Dict, List, Optional, Union, Any
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel, Field
import requests # Ensure requests is imported at the top
import traceback # For better exception logging

# Use local implementation instead of the problematic package
try:
    from modelcontextprotocol import ModelResponse, ChatMessage, ToolCall, MessageRole, ToolResult
    print("Successfully imported modelcontextprotocol")
except ImportError:
    print("Using local implementation of MCP classes (ensure local_mcp.py exists and is correct)")
    # Make sure local_mcp.py defines these classes correctly
    # Example structure for local_mcp.py:
    # from pydantic import BaseModel, Field
    # from enum import Enum
    # from typing import List, Optional, Dict, Any
    #
    # class MessageRole(str, Enum):
    #     SYSTEM = "system"
    #     USER = "user"
    #     ASSISTANT = "assistant"
    #     TOOL = "tool"
    #
    # class ChatMessage(BaseModel):
    #     role: MessageRole
    #     content: str
    #
    # class ToolCall(BaseModel):
    #     id: str
    #     type: str = "function" # MCP spec might differ, adjust if needed
    #     function: Dict[str, Any] # { name: "...", arguments: "{...}" }
    #
    # class ToolResult(BaseModel):
    #      tool_call_id: str
    #      content: str # JSON stringified result usually
    #
    # class ModelResponse(BaseModel):
    #     content: Optional[str] = None
    #     role: MessageRole = MessageRole.ASSISTANT
    #     tool_calls: Optional[List[ToolCall]] = None
    #     # Add other fields as per MCP spec if needed (e.g., finish_reason, usage)

    # --- This is just an example structure ---
    # --- Replace with your actual local_mcp.py content or ensure the package is installed ---
    try:
        from local_mcp import ModelResponse, ChatMessage, ToolCall, MessageRole, ToolResult
    except ImportError as e:
        print(f"FATAL: Could not import MCP classes from package or local_mcp.py: {e}")
        print("Please install 'modelcontextprotocol' or create a valid 'local_mcp.py'.")
        import sys
        sys.exit(1)


app = FastAPI(title="Perplexity MCP Server")

# Model configuration
class ModelConfig(BaseModel):
    model_id: str = "demo-model"
    display_name: str = "Demo Model (Proxy for Perplexity)"
    description: str = "A demonstration MCP-compatible server proxying to Perplexity API"
    capabilities: List[str] = ["chat"] # Only implementing chat for now
    max_input_tokens: int = 4096 # Example value, adjust based on proxied model
    max_total_tokens: int = 8192 # Example value, adjust based on proxied model

# Simple in-memory model registry
MODEL_REGISTRY = {
    "demo-model": ModelConfig()
}

@app.get("/")
async def root():
    return {"message": "Perplexity MCP Server is running"}

@app.get("/v1/models")
async def list_models():
    """List available models"""
    models = []
    for model_id, config in MODEL_REGISTRY.items():
        models.append({
            "id": model_id,
            "display_name": config.display_name,
            "description": config.description,
            "capabilities": config.capabilities,
            "max_input_tokens": config.max_input_tokens,
            "max_total_tokens": config.max_total_tokens
        })
    return {"models": models}

@app.get("/v1/models/{model_id}")
async def get_model(model_id: str):
    """Get model information"""
    if model_id not in MODEL_REGISTRY:
        raise HTTPException(status_code=404, detail=f"Model {model_id} not found")

    config = MODEL_REGISTRY[model_id]
    return {
        "id": model_id,
        "display_name": config.display_name,
        "description": config.description,
        "capabilities": config.capabilities,
        "max_input_tokens": config.max_input_tokens,
        "max_total_tokens": config.max_total_tokens
    }

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    stream: bool = False
    max_tokens: Optional[int] = Field(default=None, description="Maximum number of tokens to generate")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0, description="Sampling temperature between 0 and 2")
    top_p: Optional[float] = Field(default=None, ge=0.0, le=1.0, description="Nucleus sampling parameter between 0 and 1")
    top_k: Optional[int] = Field(default=None, description="Top-k sampling parameter")
    presence_penalty: Optional[float] = Field(default=None, ge=-2.0, le=2.0, description="Presence penalty between -2 and 2")
    frequency_penalty: Optional[float] = Field(default=None, ge=-2.0, le=2.0, description="Frequency penalty between -2 and 2")
    stop: Optional[Union[str, List[str]]] = Field(default=None, description="Stop sequences that cause the model to stop generating")
    # Note: Perplexity API might not support all OpenAI parameters like repetition_penalty, logprobs, tool_results directly in the same way.
    # Check Perplexity documentation for supported parameters.
    # repetition_penalty: Optional[float] = Field(default=None, description="Repetition penalty for token generation")
    # logprobs: Optional[bool] = Field(default=None, description="Whether to return log probabilities of the output tokens")
    # tool_results: Optional[List[ToolResult]] = None # Handling tool results requires more logic

@app.post("/v1/models/{model_id}/chat")
async def chat(model_id: str, request: ChatRequest):
    """Chat with the model (proxies to Perplexity API if key is present)"""
    if model_id not in MODEL_REGISTRY:
        raise HTTPException(status_code=404, detail=f"Model {model_id} not found")

    # Check if API key is available
    api_key = os.environ.get("PERPLEXITY_API_KEY")

    if not api_key:
        # --- Demo Implementation ---
        print("Warning: No Perplexity API key found. Using demo implementation.")

        user_messages = [msg for msg in request.messages if msg.role == MessageRole.USER]

        if not user_messages:
            # Return a valid ModelResponse even if no user message
            return ModelResponse(
                content="I need a user message to respond to.",
                role=MessageRole.ASSISTANT,
                tool_calls=[]
            )

        last_user_message = user_messages[-1].content

        # Log parameters (in a real implementation, these would be passed to the model)
        params = {
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "top_k": request.top_k,
            "presence_penalty": request.presence_penalty,
            "frequency_penalty": request.frequency_penalty,
            "stop": request.stop,
            # "repetition_penalty": request.repetition_penalty,
            # "logprobs": request.logprobs,
            "stream": request.stream
        }
        print(f"Demo mode. Received parameters: {params}")

        # Simple demo response
        response_content = f"You said: '{last_user_message}'. This is a demo response because PERPLEXITY_API_KEY is not set."

        return ModelResponse(
            content=response_content,
            role=MessageRole.ASSISTANT,
            tool_calls=[] # Demo doesn't support tools yet
        )

    else:
        # --- Perplexity API Implementation ---
        print(f"Using Perplexity API key: {api_key[:4]}...{api_key[-4:]}")

        # Choose a specific Perplexity model (check their docs for current options)
        # Common options: sonar-small-chat, sonar-medium-chat, sonar-small-online, sonar-medium-online
        # llama-3-sonar-small-32k-chat, llama-3-sonar-small-32k-online,
        # llama-3-sonar-large-32k-chat, llama-3-sonar-large-32k-online
        perplexity_model_name = "sonar-medium-online" # Or choose another
        print(f"Targeting Perplexity model: {perplexity_model_name}")

        perplexity_url = "https://api.perplexity.ai/chat/completions"

        # Convert MCP messages to Perplexity format (role needs to be string value)
        perplexity_messages = []
        for msg in request.messages:
            try:
                # Ensure role is the string value (e.g., "user", "assistant")
                role_value = msg.role.value if isinstance(msg.role, Enum) else str(msg.role)
                perplexity_messages.append({"role": role_value, "content": msg.content})
            except AttributeError:
                 print(f"Warning: Could not get '.value' from message role: {msg.role}. Using raw value.")
                 perplexity_messages.append({"role": str(msg.role), "content": msg.content})


        # Prepare request payload
        payload = {
            "model": perplexity_model_name,
            "messages": perplexity_messages
        }

        # Add optional parameters *if* they are provided in the request
        # Check Perplexity API docs for exact supported parameters
        if request.max_tokens is not None:
            payload["max_tokens"] = request.max_tokens
        if request.temperature is not None:
            payload["temperature"] = request.temperature
        if request.top_p is not None:
            payload["top_p"] = request.top_p
        if request.top_k is not None:
             payload["top_k"] = request.top_k # Check if Perplexity supports top_k
        if request.presence_penalty is not None:
             payload["presence_penalty"] = request.presence_penalty # Check if Perplexity supports this
        if request.frequency_penalty is not None:
             payload["frequency_penalty"] = request.frequency_penalty # Check if Perplexity supports this
        if request.stop is not None:
             payload["stop"] = request.stop # Check format (string or list)

        # MCP stream=True not handled here, always sending non-stream request
        payload["stream"] = False

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json" # Good practice to include Accept header
        }

        try:
            print(f"Sending request to Perplexity API: {perplexity_url}")
            print(f"Request payload: {json.dumps(payload, indent=2)}")

            response = requests.post(perplexity_url, headers=headers, json=payload, timeout=120) # Add timeout

            print(f"Response status code: {response.status_code}")
            # print(f"Response headers: {dict(response.headers)}") # Can be verbose
            # print(f"Raw response text: {response.text[:500]}...") # Log start of raw response for debugging

            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            result = response.json()
            # print(f"Parsed API response: {json.dumps(result, indent=2)}") # Log parsed response

            # Perplexity API standard format has 'choices' array
            if not result.get("choices"):
                print("ERROR: 'choices' field is missing or empty in the Perplexity API response.")
                # Return an error in the MCP format
                raise HTTPException(status_code=500, detail=f"Invalid response from Perplexity API: 'choices' missing. Response: {response.text[:200]}")

            first_choice = result["choices"][0]
            assistant_message = first_choice.get("message")

            if not assistant_message:
                print("ERROR: 'message' field is missing in the first choice of the Perplexity API response.")
                raise HTTPException(status_code=500, detail=f"Invalid response format from Perplexity API: 'message' missing. Response: {response.text[:200]}")

            content = assistant_message.get("content")

            if content is None:
                # Content being None might be valid if tool calls are expected, but we aren't handling them yet.
                print("WARNING: 'content' field is null or missing in the Perplexity API message. Check for tool calls if applicable.")
                # For now, return empty content if null, or handle as error if appropriate
                content = "" # Or raise HTTPException

            # Check for tool calls if you plan to support them (Perplexity might return 'tool_calls')
            # tool_calls_raw = assistant_message.get("tool_calls")
            # mcp_tool_calls = []
            # if tool_calls_raw:
            #    # Convert Perplexity tool calls to MCP ToolCall format
            #    pass # Add conversion logic here

            # Construct the MCP response
            return ModelResponse(
                content=content,
                role=MessageRole.ASSISTANT,
                tool_calls=[] # Add converted mcp_tool_calls here if supporting tools
                # Potentially add usage stats if available and needed:
                # usage=result.get("usage")
            )

        except requests.exceptions.Timeout:
            print(f"ERROR: Request to Perplexity API timed out.")
            raise HTTPException(status_code=504, detail="Request to upstream API timed out")
        except requests.exceptions.RequestException as e:
            # Handle connection errors, HTTP errors, etc.
            error_msg = f"Error calling Perplexity API: {e}"
            print(error_msg)
            # Try to get more detail from response if available
            error_detail = error_msg
            if e.response is not None:
                error_detail = f"Perplexity API Error: {e.response.status_code} - {e.response.text[:200]}"
            raise HTTPException(status_code=502, detail=error_detail) # 502 Bad Gateway might be appropriate
        except json.JSONDecodeError as e:
            print(f"ERROR: Failed to parse JSON response from Perplexity API: {e}")
            print(f"Raw response that failed parsing: {response.text[:500]}")
            raise HTTPException(status_code=500, detail=f"Failed to parse upstream API response as JSON. Raw response: {response.text[:200]}")
        except Exception as e:
            # Catch-all for other unexpected errors
            error_msg = f"An unexpected error occurred: {str(e)}"
            print(error_msg)
            traceback.print_exc() # Print full traceback for debugging
            raise HTTPException(status_code=500, detail=error_msg)


# --- Keep other endpoints as they were ---

@app.get("/api-key-test")
async def test_api_key():
    """Test if the Perplexity API key is properly set"""
    api_key = os.environ.get("PERPLEXITY_API_KEY")

    if not api_key:
        return {"status": "error", "message": "No PERPLEXITY_API_KEY environment variable found"}
    else:
        # Only show first few characters for security
        masked_key = api_key[:4] + "..." + api_key[-4:] if len(api_key) > 8 else "***"
        return {"status": "success", "message": f"PERPLEXITY_API_KEY found: {masked_key}"}


@app.post("/v1/models/{model_id}/complete")
async def complete(model_id: str, request: Dict[str, Any]):
    """Text completion endpoint (Placeholder)"""
    # NOTE: This endpoint is not fully implemented to proxy to Perplexity's completion
    # (if they have one separate from chat) or use the chat endpoint for completion.
    # Also, 'complete' capability was removed from ModelConfig as only chat is implemented.
    if model_id not in MODEL_REGISTRY:
        raise HTTPException(status_code=404, detail=f"Model {model_id} not found")

    # Re-check capability if you implement completion
    # if "complete" not in MODEL_REGISTRY[model_id].capabilities:
    #     raise HTTPException(status_code=400, detail=f"Model {model_id} does not support completion")

    # Mock completion implementation
    prompt = request.get("prompt", "")
    response = f"Completion for: '{prompt}'. This is a demo completion. API proxying not implemented for this endpoint."

    # This format might need adjustment based on expected completion response structure
    return {
        "choices": [{
            "text": response,
            "index": 0,
            "logprobs": None,
            "finish_reason": "stop"
        }],
        # Add other fields like 'id', 'object', 'created', 'model' if needed
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    # Could add checks here (e.g., try a cheap Perplexity API call if key exists)
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    import socket

    # Try to find an available port starting from 5000
    port = int(os.environ.get("PORT", 5000))
    host = os.environ.get("HOST", "0.0.0.0") # Allow configuring host
    max_port_attempts = 10
    found_port = False

    for attempt in range(max_port_attempts):
        try:
            # Try to create a socket to check if the port is available
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                 sock.bind((host, port))
            # If we get here, the port is available
            found_port = True
            break
        except OSError: # Catch OSError which includes 'address already in use'
            print(f"Port {port} on host {host} is already in use, trying {port + 1}")
            port += 1

    if not found_port:
        print(f"Could not find an available port after {max_port_attempts} attempts starting from {port - max_port_attempts}")
        import sys
        sys.exit(1)

    print(f"Starting server on http://{host}:{port}")
    # Consider adding --reload for development, but remove for production
    uvicorn.run(app, host=host, port=port)
